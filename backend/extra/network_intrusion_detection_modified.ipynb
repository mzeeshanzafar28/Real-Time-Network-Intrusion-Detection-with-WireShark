{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dfae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install catboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6257632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy==1.26.4 pandas==2.2.2 --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b08d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"hassan06/nslkdd\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720efd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "train_file_path = path + '/versions/1/KDDTrain+.txt'\n",
    "test_file_path =  path + '/versions/1/KDDTest+.txt'\n",
    "column_names = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
    "    'num_compromised', 'root_shell', 'su_attempted', 'num_root',\n",
    "    'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds',\n",
    "    'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
    "    'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
    "    'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label', 'difficulty'\n",
    "]\n",
    "\n",
    "# Define Wireshark-exportable features (manually confirmed)\n",
    "wireshark_features = [\n",
    "    'duration', \n",
    "    'protocol_type', \n",
    "    'service', \n",
    "    'src_bytes', \n",
    "    'dst_bytes', \n",
    "    'flag', \n",
    "    'land', \n",
    "    'wrong_fragment', \n",
    "    'urgent', \n",
    "    'label'  # include the label for classification\n",
    "]\n",
    "\n",
    "# Load datasets\n",
    "df_train = pd.read_csv(train_file_path, names=column_names, index_col=False)\n",
    "df_test = pd.read_csv(test_file_path, names=column_names, index_col=False)\n",
    "df_train\n",
    "# Drop non-Wireshark features\n",
    "df_train = df_train[wireshark_features]\n",
    "df_test = df_test[wireshark_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94950ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Drop the 'difficulty' column\n",
    "df_train.drop(columns=['difficulty'], inplace=True)\n",
    "df_test.drop(columns=['difficulty'], inplace=True)\n",
    "\n",
    "# Combine datasets to ensure consistent encoding\n",
    "df_combined = pd.concat([df_train, df_test])\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_features = ['protocol_type', 'service', 'flag']\n",
    "numerical_features = [col for col in df_train.columns if col not in categorical_features + ['label']]\n",
    "\n",
    "# Define preprocessing for categorical and numerical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "        ('num', StandardScaler(), numerical_features)\n",
    "    ])\n",
    "\n",
    "# Prepare features and labels\n",
    "X_combined = df_combined.drop('label', axis=1)\n",
    "y_combined = df_combined['label'].apply(lambda x: 0 if x == 'normal' else 1)  # Binary classification\n",
    "\n",
    "# Split back into train and test sets\n",
    "X_train = X_combined.iloc[:len(df_train), :]\n",
    "X_test = X_combined.iloc[len(df_train):, :]\n",
    "y_train = y_combined.iloc[:len(df_train)]\n",
    "y_test = y_combined.iloc[len(df_train):]\n",
    "\n",
    "# Fit and transform the data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdbafcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "import pickle\n",
    "\n",
    "# Initialize and train the CatBoost model\n",
    "model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, verbose=200)\n",
    "model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=10)\n",
    "\n",
    "# Save the trained model as a pickle file\n",
    "model_filename = \"catboost_model.pkl\"\n",
    "with open(model_filename, \"wb\") as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "print(f\"Model saved as {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a625cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Model Precision: {precision:.2f}\")\n",
    "print(f\"Model Recall: {recall:.2f}\")\n",
    "print(f\"Model F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c09bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b9a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the structure of your features based on the original dataset\n",
    "dummy_data = pd.DataFrame({\n",
    "    'duration': [np.random.randint(0, 1000)],\n",
    "    'protocol_type': ['tcp'],  # Example categorical value\n",
    "    'service': ['http'],       # Example categorical value\n",
    "    'flag': ['SF'],            # Example categorical value\n",
    "    'src_bytes': [np.random.randint(0, 10000)],\n",
    "    'dst_bytes': [np.random.randint(0, 10000)],\n",
    "    'land': [0],\n",
    "    'wrong_fragment': [0],\n",
    "    'urgent': [0],\n",
    "    'hot': [0],\n",
    "    'num_failed_logins': [0],\n",
    "    'logged_in': [1],\n",
    "    'num_compromised': [0],\n",
    "    'root_shell': [0],\n",
    "    'su_attempted': [0],\n",
    "    'num_root': [0],\n",
    "    'num_file_creations': [0],\n",
    "    'num_shells': [0],\n",
    "    'num_access_files': [0],\n",
    "    'num_outbound_cmds': [0],\n",
    "    'is_host_login': [0],\n",
    "    'is_guest_login': [0],\n",
    "    'count': [np.random.randint(0, 100)],\n",
    "    'srv_count': [np.random.randint(0, 100)],\n",
    "    'serror_rate': [np.random.random()],\n",
    "    'srv_serror_rate': [np.random.random()],\n",
    "    'rerror_rate': [np.random.random()],\n",
    "    'srv_rerror_rate': [np.random.random()],\n",
    "    'same_srv_rate': [np.random.random()],\n",
    "    'diff_srv_rate': [np.random.random()],\n",
    "    'srv_diff_host_rate': [np.random.random()],\n",
    "    'dst_host_count': [np.random.randint(0, 255)],\n",
    "    'dst_host_srv_count': [np.random.randint(0, 255)],\n",
    "    'dst_host_same_srv_rate': [np.random.random()],\n",
    "    'dst_host_diff_srv_rate': [np.random.random()],\n",
    "    'dst_host_same_src_port_rate': [np.random.random()],\n",
    "    'dst_host_srv_diff_host_rate': [np.random.random()],\n",
    "    'dst_host_serror_rate': [np.random.random()],\n",
    "    'dst_host_srv_serror_rate': [np.random.random()],\n",
    "    'dst_host_rerror_rate': [np.random.random()],\n",
    "    'dst_host_srv_rerror_rate': [np.random.random()]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Assuming 'preprocessor' was previously defined and fitted on the training data\n",
    "# If not, you need to fit it on the training data before transforming new data\n",
    "\n",
    "# Transform the dummy data\n",
    "dummy_data_transformed = preprocessor.transform(dummy_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f036244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_filename = 'catboost_model.pkl'  # Replace with your actual model filename\n",
    "with open(model_filename, 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8bd1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the loaded model\n",
    "predictions = loaded_model.predict(dummy_data_transformed)\n",
    "\n",
    "# Output the prediction\n",
    "print(f\"Prediction for dummy data: {predictions[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0031c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to LightGBM Dataset format\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# Define parameters for LightGBM\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_error',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "lgb_model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[valid_data],  # Pass the Dataset object for validation\n",
    ")\n",
    "\n",
    "# Save the trained LightGBM model as a pickle file\n",
    "lgb_model_filename = \"lightgbm_model.pkl\"\n",
    "with open(lgb_model_filename, \"wb\") as model_file:\n",
    "    pickle.dump(lgb_model, model_file)\n",
    "\n",
    "print(f\"LightGBM Model saved as {lgb_model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pytorch-tabnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc49b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import pickle\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Define and train the TabNet model\n",
    "tabnet_model = TabNetClassifier(\n",
    "    n_d=8,\n",
    "    n_a=8,\n",
    "    n_steps=3,\n",
    "    gamma=1.3,\n",
    "    lambda_sparse=1e-5,\n",
    "    clip_value=2.,\n",
    "    optimizer_fn=torch.optim.Adam,  # Use the actual Adam optimizer function\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    mask_type=\"entmax\"\n",
    ")\n",
    "\n",
    "# Fit the model with evaluation set in the correct format (as a list of tuples)\n",
    "tabnet_model.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "\n",
    "# Save the trained TabNet model as a pickle file\n",
    "tabnet_model_filename = \"tabnet_model.pkl\"\n",
    "with open(tabnet_model_filename, \"wb\") as model_file:\n",
    "    pickle.dump(tabnet_model, model_file)\n",
    "\n",
    "print(f\"TabNet Model saved as {tabnet_model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f28638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the trained LightGBM model from the pickle file\n",
    "with open(\"lightgbm_model.pkl\", \"rb\") as model_file:\n",
    "    loaded_lgb_model = pickle.load(model_file)\n",
    "\n",
    "# Make predictions on new data\n",
    "predictions_lgb = loaded_lgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1853303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "predictions_lgb = loaded_lgb_model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to binary predictions using a threshold of 0.5\n",
    "y_pred = (predictions_lgb >= 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Model Precision: {precision:.2f}\")\n",
    "print(f\"Model Recall: {recall:.2f}\")\n",
    "print(f\"Model F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae68bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained TabNet model from the pickle file\n",
    "with open(\"tabnet_model.pkl\", \"rb\") as model_file:\n",
    "    loaded_tabnet_model = pickle.load(model_file)\n",
    "\n",
    "# Make predictions on new data\n",
    "predictions_tabnet = loaded_tabnet_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31d6cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = loaded_tabnet_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Model Precision: {precision:.2f}\")\n",
    "print(f\"Model Recall: {recall:.2f}\")\n",
    "print(f\"Model F1 Score: {f1:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
